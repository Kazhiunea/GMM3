# -*- coding: utf-8 -*-
"""3uzduotisRemake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pje8QqiYK90OJi8IdkEfRRVntF-2AhHO
"""

import urllib.request,sys,time
from bs4 import BeautifulSoup
import requests
import pandas as pd
import numpy as np
import time
import re
import torch

from google.colab import drive

import urllib.request,sys,time
from bs4 import BeautifulSoup
import requests
import pandas as pd
import numpy as np
import time
import re
import torch

#Drive
from google.colab import drive

#Dataset
from datasets import Dataset

#Transforms
import torch
from transformers import ReformerModel, ReformerTokenizer

#Importing pada

drive.mount('/content/gdrive', force_remount=True)

"""# Scraping

## Gettings links
"""

months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']
links = []
titles = []
regex = "(0?[1-9]|1[012])[\/\-\.](0?[1-9]|[12][0-9]|3[01])[\/\-\.]\d{4}(?: - )"

for year in range(2016,2023):
  for month in months:
    URL = f"https://www.theonion.com/sitemap/{year}/{month}"
    page = requests.get(URL)
    soup = BeautifulSoup(page.text, "html.parser")
    stories = soup.find_all("h4")
    for a in stories:
      # print(a.a['href'])
      links.append(a.a['href'])
      titles.append(re.sub(regex,"",a.text))


print(len(titles),len(links))

def addToDict(dictionary,url,title="",text="",theme=""):
  dictionary["url"].append(url)
  dictionary["title"].append(title)
  dictionary["text"].append(text)
  dictionary["theme"].append(theme)

"""## Scraping websites for title, theme

"""

from bs4.element import SoupStrainer
import concurrent.futures
import gc

MAX_THREADS = 30

data = {
    "url": [],
    "title": [],
    "text": [],
    "theme": []
}

def download_url(urlx):
    url,title = urlx
    fullText = ''
    resp = requests.get(url)
    article_content = resp.content
    soup_article = BeautifulSoup(article_content, 'lxml', parse_only=SoupStrainer(['p','span']))
    textSoup = soup_article.find_all("p", attrs={"class":"sc-77igqf-0"})
    themeSoup = soup_article.find("span",attrs={"class":"ov8naf-0 jSfxTR"})
    theme = themeSoup.text
    if textSoup:
      fullText = textSoup[0].text
    addToDict(data,url,text=fullText, title=title, theme=theme)
    soup_article.decompose()
    soup_article = None
    textSoup = None
    themeSoup = None
    fullText = None
    article_content = None
    resp = None
    gc.collect()
    
def download_stories(story_urls):
    threads = min(MAX_THREADS, len(story_urls))
    t0 = time.time()
    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
        executor.map(download_url, story_urls)
    t1 = time.time()
    print(f"{t1-t0} seconds to download {len(story_urls)} stories.")

urlTitle = list(zip(links,titles))
download_stories(urlTitle[:13000])
# download_url("https://www.theonion.com/stephen-miller-rewards-self-after-day-of-speechwriting-1822562453")

df = pd.DataFrame(data)
df['text'].replace('', np.nan, inplace=True)
df.dropna(subset=['text'], inplace=True)
df.drop_duplicates(subset=['text'], keep="last", inplace=True)
df.to_csv("/content/gdrive/MyDrive/GMM3/dict.csv", sep=";", encoding='utf-8-sig', index=False)

"""# Transofmring data"""

!pip install datasets 
!pip install faiss-gpu
!pip install transformers
!pip install sentencepiece

duomenis = pd.read_csv("/content/gdrive/MyDrive/GMM3/dict.csv", sep=";")

duomenis['combined'] = duomenis['title'] + ' ' + duomenis['text']

from datasets import Dataset

comments_dataset = Dataset.from_pandas(duomenis)
comments_dataset

import torch
from transformers import ReformerModel, ReformerTokenizer

tokenizer = ReformerTokenizer.from_pretrained("google/reformer-crime-and-punishment")
model = ReformerModel.from_pretrained("google/reformer-crime-and-punishment")

device = torch.device("cuda")
model.to(device)

def cls_pooling(model_output):
    return model_output.last_hidden_state[:, -1]

def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=False, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)

embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape

tokenizer.add_special_tokens({'pad_token': '[PAD]'})

embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["combined"]).detach().cpu().numpy()[0]}
)

embeddings_dataset['embeddings'][:50]

embeddings_dataset.to_csv("/content/gdrive/MyDrive/GMM3/dataset.csv")

embeddings_dataset.add_faiss_index(column="embeddings")

embeddings_dataset['embeddings'][:50]

question = "China Debuts ‘Straddling Bus’"
question_embedding = get_embeddings([question]).cpu().detach().numpy()[0]
question_embedding.shape

embeddings_dataset.shape

question_embedding

scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=10
)

question_embedding

print(samples['url'])

import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)

for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.text}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.url}")
    print("=" * 50)
    print()

!pip install flask-ngrok

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz
!tar -xvf /content/ngrok-stable-linux-amd64.tgz
!./ngrok authtoken 1sSAZYr9xeJs5v4iHh2AExerSbk_6E5PKipDw73vjTy3GMqNo

from flask import Flask
from flask_ngrok import run_with_ngrok
from flask import request

app = Flask(__name__)
run_with_ngrok(app)   
@app.route("/")
def home():
    return "<h1> REST Api resnet50 Panda,snake or scissors clasiffier </h2>"
    
@app.route("/",methods=['POST'])
def checkClose():
    if request.method == 'POST':
        data = request.form['input']
        score,samples = getClosest(data)
        print(samples)
        print(score)
        return f"URL: {samples['url']} \n| Text: {samples['text']} \n| Score: {score}"

app.run()

def getClosest(text):
  question = text
  question_embedding = get_embeddings([question]).cpu().detach().numpy()
  question_embedding.shape
  scores, samples = embeddings_dataset.get_nearest_examples(
      "embeddings", question_embedding, k=1
  )
  return (scores,samples)